{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "We first define a function for downloading and loading MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(train_dir = \"./MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Implementation\n",
    "Using two convolutional layers followed by relu activation and max pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    \"\"\"predict returns prediction given input.\"\"\"\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    W_conv1 = self.weight_variable([3, 3, 1, 16])\n",
    "    b_conv1 = self.bias_variable([16])\n",
    "    h_conv1 = tf.nn.relu(self.conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "\n",
    "    W_conv2 = self.weight_variable([3, 3, 16, 16])\n",
    "    b_conv2 = self.bias_variable([16])\n",
    "    h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    h_pool2 = self.max_pool_2x2(h_conv2)\n",
    "\n",
    "    W_fc1 = self.weight_variable([7 * 7 * 16, 128])\n",
    "    b_fc1 = self.bias_variable([128])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    W_fc2 = self.weight_variable([128, 10])\n",
    "    b_fc2 = self.bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "    return y_conv\n",
    "\n",
    "\n",
    "def conv2d(self, x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(self, x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(self, shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(self, shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def train(learning_rate=0.01, max_epochs=1000, \n",
    "              batch_size=64):\n",
    "        \"\"\" Train network on the given data. \"\"\"\n",
    "        \n",
    "        # Define placeholder for x\n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "        # Define placeholder for y\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Predict given the data\n",
    "        y_conv = self.predict(x)\n",
    "\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n",
    "                                                                logits=y_conv)\n",
    "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "        # Define loss and optimizer\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction)\n",
    "        \n",
    "        train_acc = list()\n",
    "        val_acc = list()\n",
    "        test_acc = list()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(max_epochs):\n",
    "                for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                    train_step.run(feed_dict={x: batch_xs, y_: batch_ys})\n",
    "                \n",
    "                print('Epoch', epoch, 'completed out of', max_epochs)\n",
    "            \n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                        x: mnist.train.images, y_: mnist.train.labels})\n",
    "                \n",
    "                validation_accuracy = accuracy.eval(feed_dict={\n",
    "                    x: mnist.eval.images, y_: mnist.eval.labels})\n",
    "                \n",
    "                test_accuracy = accuracy.eval(feed_dict={\n",
    "                    x: mnist.test.images, y_: mnist.test.labels})\n",
    "                \n",
    "                train_acc.append(train_accuracy)\n",
    "                val_acc.append(validation_accuracy)\n",
    "                test_acc.append(test_accuracy)\n",
    "                \n",
    "#                 print('epoch %d, training accuracy %g' % (epoch, train_accuracy))\n",
    "#                 print('epoch %d, validation accuracy %g' % (epoch, train_accuracy))\n",
    "#                 print('epoch %d, test accuracy %g' % (epoch, train_accuracy))\n",
    "                \n",
    "            return (train_acc, val_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST\n",
    "Finally we can let our network run on the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Changing the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "validation_errors = list()\n",
    "for lr in learning_rates:\n",
    "    validation_errors.append(train(learning_rate=lr)[1])\n",
    "\n",
    "for lr, ve in zip(learning_rates, validation_errors):\n",
    "    plt.plot(range(1,len(ve)), ve, label='Learning rate:%s' % lr)\n",
    "    plt.xlabel('number of epochs')\n",
    "    plt.ylabel('validation error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
